# NLLB-200 Fine-tuned pour la traduction FranÃ§ais-Wolof ğŸ‡«ğŸ‡·â†”ï¸ğŸ‡¸ğŸ‡³

[![Hugging Face Model](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Model-yellow)](https://huggingface.co/Lahad/nllb200-francais-wolof)

ModÃ¨le NLLB-200 (version distillÃ©e 600M) fine-tunÃ© pour la traduction FranÃ§ais â†’ Wolof.

---

## ğŸ“¥ Installation

```bash
pip install transformers datasets
```

## ğŸš€ Utilisation rapide

### Chargement du modÃ¨le

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Lahad/nllb200-francais-wolof")
model = AutoModelForSeq2SeqLM.from_pretrained("Lahad/nllb200-francais-wolof")
```

### Fonction de traduction

```python
def translate(text, max_length=128):
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        forced_bos_token_id=tokenizer.convert_tokens_to_ids("wol_Latn"),
        max_length=max_length
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### Exemples

```python
print(translate("Bonjour comment Ã§a va ?"))  # Nanga def ?
print(translate("Jâ€™ai dÃ©cidÃ© de quitter mon boulot"))  # Dama jÃ«l dogal ni damay bÃ yyi sama liggÃ©ey
```

## ğŸ§  Configuration technique

### Architecture

* **ModÃ¨le de base:** `facebook/nllb-200-distilled-600M`
* **ParamÃ¨tres:** 600M
* **Context Window:** 128 tokens
* **Langues:**
   * Source: `fr_Latn` (FranÃ§ais)
   * Cible: `wol_Latn` (Wolof)

### HyperparamÃ¨tres d'entraÃ®nement

```python
Seq2SeqTrainingArguments(
    output_dir="./results_nllb_finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    fp16=True,
    evaluation_strategy="epoch"
)
```

## ğŸ“Š DonnÃ©es

* **Dataset:** galsenai/centralized_wolof_french_translation_data
* **Split:** 80/20 (Train/Test)
* **PrÃ©processing:**
   * Tokenization avec padding dynamique
   * Longueur maximale: 128 tokens
   * Format:

```json
{"fr": "Texte franÃ§ais", "wo": "Traduction wolof"}
```

## ğŸ“ˆ Performances

| MÃ©trique | Valeur |
|----------|--------|
| Loss final | 1.23 |
| BLEU Score | 42.1 |
| Temps d'entraÃ®nement | 5h (T4) |

## âš ï¸ Limitations

1. Performances rÃ©duites sur textes techniques
2. LimitÃ© Ã  128 tokens par dÃ©faut

## ğŸ“œ Licence

* **ModÃ¨le:** CC-BY-NC-4.0
* **DonnÃ©es:** Licence originale du dataset

## ğŸ™ CrÃ©dits

* Meta AI pour NLLB-200
* GalsenAI pour les donnÃ©es
* CommunautÃ© Hugging Face ğŸ¤—


